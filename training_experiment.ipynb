{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lardex\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nbformat import read, NO_CONVERT\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from random import randint\n",
    "from pickle import dump, load\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "# Load SpaCy large English model and disable unnecessary components\n",
    "nlp = spacy.load('en_core_web_lg', disable=[\"tagger\", \"ner\", \"lemmatizer\"])\n",
    "\n",
    "# Function to remove punctuation\n",
    "def separate_punc(md_text):\n",
    "    return [token.text.lower() for token in nlp(md_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n']\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV into DataFrame\n",
    "df = pd.read_csv('extracted_code.csv')\n",
    "\n",
    "# Tokenize the code from the DataFrame\n",
    "tokens = []\n",
    "for code in df['code']:\n",
    "    tokens.extend(separate_punc(code))\n",
    "\n",
    "# Generate sequences of tokens\n",
    "train_len = 26\n",
    "text_sequences = [tokens[i-train_len:i] for i in range(train_len, len(tokens))]\n",
    "\n",
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "\n",
    "# Convert text sequences to numerical sequences\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
    "num_sequences = np.array(sequences)\n",
    "\n",
    "# Prepare input and output variables\n",
    "X = num_sequences[:,:-1]\n",
    "y = num_sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=len(tokenizer.word_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Lardex\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lardex\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 25, 25)            268450    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25, 150)           105600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               180600    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               22650     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10738)             1621438   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2198738 (8.39 MB)\n",
      "Trainable params: 2198738 (8.39 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:From c:\\Users\\Lardex\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Lardex\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1534/1534 [==============================] - 127s 79ms/step - loss: 6.7919 - accuracy: 0.0873\n",
      "Epoch 2/100\n",
      "1534/1534 [==============================] - 116s 76ms/step - loss: 6.0486 - accuracy: 0.1266\n",
      "Epoch 3/100\n",
      "1534/1534 [==============================] - 116s 75ms/step - loss: 5.4112 - accuracy: 0.1690\n",
      "Epoch 4/100\n",
      "1534/1534 [==============================] - 122s 79ms/step - loss: 4.8713 - accuracy: 0.2034\n",
      "Epoch 5/100\n",
      "1534/1534 [==============================] - 123s 80ms/step - loss: 4.4060 - accuracy: 0.2367\n",
      "Epoch 6/100\n",
      "1534/1534 [==============================] - 113s 73ms/step - loss: 4.0055 - accuracy: 0.2711\n",
      "Epoch 7/100\n",
      "1534/1534 [==============================] - 113s 74ms/step - loss: 3.6239 - accuracy: 0.3107\n",
      "Epoch 8/100\n",
      "1534/1534 [==============================] - 113s 74ms/step - loss: 3.2912 - accuracy: 0.3478\n",
      "Epoch 9/100\n",
      "1534/1534 [==============================] - 114s 74ms/step - loss: 3.0217 - accuracy: 0.3841\n",
      "Epoch 10/100\n",
      "1534/1534 [==============================] - 119s 77ms/step - loss: 2.8284 - accuracy: 0.4105\n",
      "Epoch 11/100\n",
      "1534/1534 [==============================] - 115s 75ms/step - loss: 2.5856 - accuracy: 0.4484\n",
      "Epoch 12/100\n",
      "1534/1534 [==============================] - 115s 75ms/step - loss: 2.4131 - accuracy: 0.4762\n",
      "Epoch 13/100\n",
      "1534/1534 [==============================] - 114s 74ms/step - loss: 2.2622 - accuracy: 0.5013\n",
      "Epoch 14/100\n",
      "1534/1534 [==============================] - 114s 74ms/step - loss: 2.1329 - accuracy: 0.5233\n",
      "Epoch 15/100\n",
      "1534/1534 [==============================] - 114s 74ms/step - loss: 2.0230 - accuracy: 0.5458\n",
      "Epoch 16/100\n",
      "1534/1534 [==============================] - 114s 74ms/step - loss: 1.9104 - accuracy: 0.5661\n",
      "Epoch 17/100\n",
      "1534/1534 [==============================] - 118s 77ms/step - loss: 1.8209 - accuracy: 0.5835\n",
      "Epoch 18/100\n",
      "1534/1534 [==============================] - 112s 73ms/step - loss: 1.7460 - accuracy: 0.5979\n",
      "Epoch 19/100\n",
      "1534/1534 [==============================] - 114s 74ms/step - loss: 1.6690 - accuracy: 0.6122\n",
      "Epoch 20/100\n",
      "1534/1534 [==============================] - 113s 74ms/step - loss: 1.5933 - accuracy: 0.6277\n",
      "Epoch 21/100\n",
      "1534/1534 [==============================] - 113s 73ms/step - loss: 1.5188 - accuracy: 0.6434\n",
      "Epoch 22/100\n",
      "1534/1534 [==============================] - 115s 75ms/step - loss: 1.4563 - accuracy: 0.6564\n",
      "Epoch 23/100\n",
      "1534/1534 [==============================] - 113s 74ms/step - loss: 1.3987 - accuracy: 0.6684\n",
      "Epoch 24/100\n",
      "1534/1534 [==============================] - 113s 74ms/step - loss: 1.3574 - accuracy: 0.6765\n",
      "Epoch 25/100\n",
      "1534/1534 [==============================] - 109s 71ms/step - loss: 1.2890 - accuracy: 0.6913\n",
      "Epoch 26/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 1.2375 - accuracy: 0.7030\n",
      "Epoch 27/100\n",
      "1534/1534 [==============================] - 103s 67ms/step - loss: 1.1950 - accuracy: 0.7107\n",
      "Epoch 28/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 1.1518 - accuracy: 0.7202\n",
      "Epoch 29/100\n",
      "1534/1534 [==============================] - 102s 67ms/step - loss: 1.1003 - accuracy: 0.7326\n",
      "Epoch 30/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 1.0574 - accuracy: 0.7408\n",
      "Epoch 31/100\n",
      "1534/1534 [==============================] - 108s 70ms/step - loss: 1.0384 - accuracy: 0.7455\n",
      "Epoch 32/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.9798 - accuracy: 0.7590\n",
      "Epoch 33/100\n",
      "1534/1534 [==============================] - 108s 70ms/step - loss: 0.9435 - accuracy: 0.7662\n",
      "Epoch 34/100\n",
      "1534/1534 [==============================] - 108s 70ms/step - loss: 0.9082 - accuracy: 0.7731\n",
      "Epoch 35/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.8966 - accuracy: 0.7756\n",
      "Epoch 36/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.8506 - accuracy: 0.7872\n",
      "Epoch 37/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.8221 - accuracy: 0.7940\n",
      "Epoch 38/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.8023 - accuracy: 0.7982\n",
      "Epoch 39/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.7641 - accuracy: 0.8067\n",
      "Epoch 40/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.7319 - accuracy: 0.8130\n",
      "Epoch 41/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.7043 - accuracy: 0.8202\n",
      "Epoch 42/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.6779 - accuracy: 0.8261\n",
      "Epoch 43/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.6572 - accuracy: 0.8311\n",
      "Epoch 44/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.6307 - accuracy: 0.8374\n",
      "Epoch 45/100\n",
      "1534/1534 [==============================] - 106s 69ms/step - loss: 0.6032 - accuracy: 0.8436\n",
      "Epoch 46/100\n",
      "1534/1534 [==============================] - 110s 72ms/step - loss: 0.5891 - accuracy: 0.8472\n",
      "Epoch 47/100\n",
      "1534/1534 [==============================] - 108s 70ms/step - loss: 0.5591 - accuracy: 0.8546\n",
      "Epoch 48/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.5825 - accuracy: 0.8481\n",
      "Epoch 49/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.5284 - accuracy: 0.8611\n",
      "Epoch 50/100\n",
      "1534/1534 [==============================] - 108s 70ms/step - loss: 0.7052 - accuracy: 0.8233\n",
      "Epoch 51/100\n",
      "1534/1534 [==============================] - 107s 70ms/step - loss: 0.6728 - accuracy: 0.8257\n",
      "Epoch 52/100\n",
      "1534/1534 [==============================] - 102s 67ms/step - loss: 0.5438 - accuracy: 0.8582\n",
      "Epoch 53/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.5282 - accuracy: 0.8619\n",
      "Epoch 54/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.4963 - accuracy: 0.8691\n",
      "Epoch 55/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.4818 - accuracy: 0.8727\n",
      "Epoch 56/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.4861 - accuracy: 0.8694\n",
      "Epoch 57/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.4595 - accuracy: 0.8789\n",
      "Epoch 58/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.4407 - accuracy: 0.8826\n",
      "Epoch 59/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.4144 - accuracy: 0.8900\n",
      "Epoch 60/100\n",
      "1534/1534 [==============================] - 102s 67ms/step - loss: 0.3996 - accuracy: 0.8919\n",
      "Epoch 61/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.3839 - accuracy: 0.8968\n",
      "Epoch 62/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.3752 - accuracy: 0.8983\n",
      "Epoch 63/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.3659 - accuracy: 0.9004\n",
      "Epoch 64/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.3694 - accuracy: 0.8992\n",
      "Epoch 65/100\n",
      "1534/1534 [==============================] - 102s 67ms/step - loss: 0.3680 - accuracy: 0.8982\n",
      "Epoch 66/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.3393 - accuracy: 0.9073\n",
      "Epoch 67/100\n",
      "1534/1534 [==============================] - 102s 67ms/step - loss: 0.3613 - accuracy: 0.9023\n",
      "Epoch 68/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.3029 - accuracy: 0.9177\n",
      "Epoch 69/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.3615 - accuracy: 0.9015\n",
      "Epoch 70/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.3054 - accuracy: 0.9161\n",
      "Epoch 71/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.2835 - accuracy: 0.9225\n",
      "Epoch 72/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.2772 - accuracy: 0.9226\n",
      "Epoch 73/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.2808 - accuracy: 0.9212\n",
      "Epoch 74/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.2729 - accuracy: 0.9244\n",
      "Epoch 75/100\n",
      "1534/1534 [==============================] - 108s 71ms/step - loss: 0.2618 - accuracy: 0.9273\n",
      "Epoch 76/100\n",
      "1534/1534 [==============================] - 102s 67ms/step - loss: 0.2491 - accuracy: 0.9308\n",
      "Epoch 77/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.2572 - accuracy: 0.9274\n",
      "Epoch 78/100\n",
      "1534/1534 [==============================] - 103s 67ms/step - loss: 0.2320 - accuracy: 0.9354\n",
      "Epoch 79/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.2564 - accuracy: 0.9283\n",
      "Epoch 80/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.2390 - accuracy: 0.9330\n",
      "Epoch 81/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.2305 - accuracy: 0.9350\n",
      "Epoch 82/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.2324 - accuracy: 0.9341\n",
      "Epoch 83/100\n",
      "1534/1534 [==============================] - 102s 67ms/step - loss: 0.2494 - accuracy: 0.9283\n",
      "Epoch 84/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.1988 - accuracy: 0.9446\n",
      "Epoch 85/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.2035 - accuracy: 0.9424\n",
      "Epoch 86/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.2722 - accuracy: 0.9262\n",
      "Epoch 87/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.1841 - accuracy: 0.9485\n",
      "Epoch 88/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.1891 - accuracy: 0.9460\n",
      "Epoch 89/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.1900 - accuracy: 0.9456\n",
      "Epoch 90/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.1852 - accuracy: 0.9469\n",
      "Epoch 91/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.1890 - accuracy: 0.9464\n",
      "Epoch 92/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.2097 - accuracy: 0.9401\n",
      "Epoch 93/100\n",
      "1534/1534 [==============================] - 100s 65ms/step - loss: 0.1836 - accuracy: 0.9463\n",
      "Epoch 94/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.1949 - accuracy: 0.9439\n",
      "Epoch 95/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.1771 - accuracy: 0.9492\n",
      "Epoch 96/100\n",
      "1534/1534 [==============================] - 102s 66ms/step - loss: 0.1768 - accuracy: 0.9495\n",
      "Epoch 97/100\n",
      "1534/1534 [==============================] - 100s 65ms/step - loss: 0.1601 - accuracy: 0.9539\n",
      "Epoch 98/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.1716 - accuracy: 0.9500\n",
      "Epoch 99/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.1777 - accuracy: 0.9490\n",
      "Epoch 100/100\n",
      "1534/1534 [==============================] - 101s 66ms/step - loss: 0.1599 - accuracy: 0.9543\n"
     ]
    }
   ],
   "source": [
    "# Create and compile the model\n",
    "model = create_model(len(tokenizer.word_index)+1, X.shape[1])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, batch_size=100, epochs=100, verbose=1)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save('notebooks_code_model_300.keras')\n",
    "dump(tokenizer, open('notebooks_code_tokenizer_300', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text\n",
    "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for _ in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
    "        pred_w = model.predict(pad_encoded, verbose=0)[0]\n",
    "        pred_word_ind = np.argmax(pred_w, axis=-1)\n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text += ' ' + pred_word\n",
    "        output_text.append(pred_word)\n",
    "    return ' '.join(output_text)\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = load_model('notebooks_code_model_300.keras')\n",
    "tokenizer = load(open('notebooks_code_tokenizer_300', 'rb'))\n",
    "\n",
    "# Test the text generation\n",
    "random_seed_text = ' '.join(text_sequences[randint(0, len(text_sequences))])\n",
    "print(generate_text(model, tokenizer, X.shape[1], random_seed_text, 25))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
